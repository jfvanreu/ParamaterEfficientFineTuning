{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b0a7a",
   "metadata": {},
   "source": [
    "In this project, we'll create a language detection model using a parameter-efficient model. This model will leverage the **DistillBert** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRa\n",
    "* Model: distilbert\n",
    "* Evaluation approach: Accuracy\n",
    "* Fine-tuning dataset: papluca/language-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5447a74",
   "metadata": {},
   "source": [
    "### Download Language Detection dataset\n",
    "We download the Language Detection dataset. We download 3 data splits: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 4.99k/4.99k [00:00<00:00, 5.23MB/s]\n",
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  35%|███▌      | 4.19M/12.0M [00:00<00:00, 11.5MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 12.0M/12.0M [00:00<00:00, 19.8MB/s]\u001b[A\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.63it/s]\n",
      "Downloading data:   0%|          | 0.00/1.71M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 1.71M/1.71M [00:00<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  2.90it/s]\n",
      "Downloading data:   0%|          | 0.00/1.69M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 1.69M/1.69M [00:00<00:00, 8.60MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  3.07it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1068.43it/s]\n",
      "Generating train split: 140000 examples [00:00, 224568.57 examples/s]\n",
      "Generating validation split: 20000 examples [00:00, 189686.66 examples/s]\n",
      "Generating test split: 20000 examples [00:00, 223881.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the datasets and transformers packages\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"papluca/language-identification\")\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(5000))\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6101b5",
   "metadata": {},
   "source": [
    "Let's take a peak at the first 5 records of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b4a149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['el', 'de', 'bg', 'de', 'vi'],\n",
       " 'text': ['Και, φυσικά, τα σπουδαία μνημεία της κατανόησης της ελευθερίας από τον δέκατο όγδοο αιώνα είναι το Σύνταγμα και ο Νόμος των Δικαιωμάτων.',\n",
       "  'Wie schon jemand bemängelte, hat die gelieferte Sitzauflage KEINE Ecken- und keine seitliche Randabdeckung, wie auf dem Bild. Auch ist die Sitzauflage entweder mit nur sehr wenig oder gar keiner Bambus-Holzkohle gefüllt, verglichen mit den Auflagen, die ich früher bei einem anderen Anbieter bestellt habe.',\n",
       "  'е като колекция от кибрити',\n",
       "  'Sehr schöne Kugel. Am 10. Oktober gekauft und am 20. Oktober geliefert 😳 Als nette Zugabe befand sich noch ein Deichmann Prospekt in meinem Päckchen 😳 Primavera und Deichmann ? Muss ich das verstehen? Für die Lieferzeit und den konventionellen Prospekt jeweils 1 1/2 Punktabzüge . Werde dort sicher nicht mehr bestellen',\n",
       "  'Một vài tầng đang cháy có lẽ nằm ngoài khả năng dập tắt của đội cứu hỏa mà chúng ta có thể xử lý.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds['test'][:5]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78e9bb",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We import a tokenizer to tokenize the text data provided as input. We also take a sample record to verify that the tokenized data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 61.0kB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 2.40MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 39.7MB/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:09<00:00, 547.84 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:06<00:00, 719.99 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:06<00:00, 782.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([  101,  1181, 15290, 29744,  1184, 10260,  1196, 29746, 14150, 29745,\n",
      "        15290, 19865, 25529, 10260, 29745,  1197, 16856, 10325, 29746,  1188,\n",
      "        29436, 10325,  1194, 15290, 18947, 22919, 10260, 29741, 14150, 19865,\n",
      "         1010,  1193, 29746, 10325, 29747, 10260, 29750,  1195, 10260, 29740,\n",
      "        14150, 22919, 10260, 22919, 10260,  1188,  1197, 10260, 29745,  1010,\n",
      "         1192, 10260,  1186, 15290, 19865, 22919, 10260,  1182,  1189, 15290,\n",
      "        29436, 10325,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=False)\n",
    "\n",
    "# add pad_token to this tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "str_to_int={\"ar\": 0, \"bg\":1, \"de\":2, \"el\":3, \"en\":4, \"es\":5, \"fr\":6, \"hi\":7, \"it\":8, \"ja\":9, \"nl\":10, \\\n",
    "              \"pl\":11, \"pt\":12, \"ru\":13, \"sw\":14, \"th\":15, \"tr\":16, \"ur\":17, \"vi\":18, \"zh\":19}\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"Preprocess the dataset by returning tokenized examples.\"\"\"\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_batch[\"labels\"] = [str_to_int[label] for label in batch[\"labels\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "    tokenized_ds[split] = tokenized_ds[split].rename_column(\"labels\", \"label\")\n",
    "\n",
    "#converting dataset to Torch Tensor and define expected col labels. This is needed to train the Lora model.\n",
    "tokenized_ds['train'].set_format('torch', columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_ds['test'].set_format('torch', columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "# Show the first example of the tokenized training set\n",
    "print(tokenized_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01095c86",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "We import the transformer model. In our case, we decided to use the **DistillBert** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 268M/268M [00:01<00:00, 222MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=20, # because we have 20 languages\n",
    "    id2label={0: \"ar\", 1: \"bg\", 2: \"de\", 3:\"el\", 4:\"en\", 5:\"es\", 6:\"fr\", 7:\"hi\", 8:\"it\", 9:\"ja\", 10:\"nl\", \\\n",
    "              11: \"pl\", 12:\"pt\", 13:\"ru\", 14:\"sw\", 15:\"th\", 16:\"tr\", 17:\"ur\", 18:\"vi\", 19:\"zh\"}, \n",
    "    label2id={\"ar\": 0, \"bg\":1, \"de\":2, \"el\":3, \"en\":4, \"es\":5, \"fr\":6, \"hi\":7, \"it\":8, \"ja\":9, \"nl\":10, \\\n",
    "              \"pl\":11, \"pt\":12, \"ru\":13, \"sw\":14, \"th\":15, \"tr\":16, \"ur\":17, \"vi\":18, \"zh\":19}\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e846c6",
   "metadata": {},
   "source": [
    "### Let's assess the pre-trained model using the accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0def71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate base model\n",
    "import numpy as np\n",
    "from transformers import Trainer, DataCollatorWithPadding, TrainingArguments\n",
    "\n",
    "#compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./regular\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# eval loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f44f3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.9903082847595215,\n",
       " 'eval_accuracy': 0.0624,\n",
       " 'eval_runtime': 52.7149,\n",
       " 'eval_samples_per_second': 94.85,\n",
       " 'eval_steps_per_second': 11.856}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf8307",
   "metadata": {},
   "source": [
    "We can see that the accuracy of the pre-trained model is pretty low (**6.24%**). Let's fine-tune the model using the LoRa approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43391058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,658,536 || all params: 69,984,552 || trainable%: 5.227633664069179\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_lin', 'k_lin','v_lin', 'lin1', 'lin2', 'classifier', 'pre-classifier'],\n",
    "    bias=\"lora_only\",\n",
    "    modules_to_save=[\"decode_head\"],\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3acb1e6",
   "metadata": {},
   "source": [
    "We can see that only **5.23%** of the parameters will be trained using the LoRa approach. This is much more efficient than re-training all the weights. Below we can see which parameters will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7edb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.pre_classifier.original_module.weight torch.Size([768, 768])\n",
      "base_model.model.pre_classifier.original_module.bias torch.Size([768])\n",
      "base_model.model.pre_classifier.modules_to_save.default.weight torch.Size([768, 768])\n",
      "base_model.model.pre_classifier.modules_to_save.default.bias torch.Size([768])\n",
      "base_model.model.classifier.original_module.weight torch.Size([20, 768])\n",
      "base_model.model.classifier.original_module.bias torch.Size([20])\n",
      "base_model.model.classifier.original_module.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.classifier.original_module.lora_B.default.weight torch.Size([20, 32])\n",
      "base_model.model.classifier.modules_to_save.default.weight torch.Size([20, 768])\n",
      "base_model.model.classifier.modules_to_save.default.bias torch.Size([20])\n",
      "base_model.model.classifier.modules_to_save.default.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.classifier.modules_to_save.default.lora_B.default.weight torch.Size([20, 32])\n"
     ]
    }
   ],
   "source": [
    "# confirm that only the LoRa paramaters are trainable\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 09:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.099619</td>\n",
       "      <td>0.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.092524</td>\n",
       "      <td>0.986200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.12442059442400932, metrics={'train_runtime': 568.6428, 'train_samples_per_second': 17.586, 'train_steps_per_second': 2.198, 'total_flos': 1414126275932160.0, 'train_loss': 0.12442059442400932, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the lora model\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/distillbert-languages-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,#4,\n",
    "    per_device_eval_batch_size=8,#2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"distillbert-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning.\n",
    "\n",
    "Let's load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distillbert-lora\", num_labels=20).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144dd887",
   "metadata": {},
   "source": [
    "Let's create a new trainer with the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4439598",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_loaded_model = Trainer(\n",
    "model = lora_model,\n",
    "tokenizer=tokenizer,\n",
    "eval_dataset=tokenized_ds['test'],\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebfa78",
   "metadata": {},
   "source": [
    "Let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ebfc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09252355992794037,\n",
       " 'eval_accuracy': 0.9862,\n",
       " 'eval_runtime': 67.6795,\n",
       " 'eval_samples_per_second': 73.878,\n",
       " 'eval_steps_per_second': 9.235}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_loaded_model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977aa189",
   "metadata": {},
   "source": [
    "We can see that the PEFT model is performing much better than the pre-trained model. As a reminder, it only had a **6.24%** accuracy while the PEFT model shows a **98.6%** accuracy. Arguably, we didn't actually train the pre-trained model, we only used it as is to detect the languages. However, by using the LoRa approach, we managed to train the model without having to update all the parameters. We only updated **5%** of those parameters. So, this is a major improvement and at low cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cfab0",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Let's apply the PEFT model on a couple of random examples:\n",
    "* \"It's a beautiful day in the park today.\" (English)\n",
    "* \"Je pense, donc je suis.\" (French)\n",
    "* \"Vamos a la playa!\" (Spanish)\n",
    "* \"Goede morgen, I ben moe\" (Dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e127452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"'It's a beautiful day in the park today.\",\n",
    "               \"Je pense, donc je suis\",\n",
    "               \"Vamos a la playa!\",\n",
    "               \"Goede morgen, I ben moe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "fr\n",
      "es\n",
      "nl\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "id2label={0: \"ar\", 1: \"bg\", 2: \"de\", 3:\"el\", 4:\"en\", 5:\"es\", 6:\"fr\", 7:\"hi\", 8:\"it\", 9:\"ja\", 10:\"nl\", \\\n",
    "              11: \"pl\", 12:\"pt\", 13:\"ru\", 14:\"sw\", 15:\"th\", 16:\"tr\", 17:\"ur\", 18:\"vi\", 19:\"zh\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=False)\n",
    "\n",
    "for test_string in test_strings:\n",
    "    input_ids = tokenizer(test_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = lora_model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "    predicted_label_classes = logits.argmax(-1)\n",
    "    print(id2label[predicted_label_classes.squeeze().tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f5179",
   "metadata": {},
   "source": [
    "Awesome! We can see that the loaded model provides the right predictions for the 4 strings that we provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
