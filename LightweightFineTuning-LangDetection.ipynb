{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b0a7a",
   "metadata": {},
   "source": [
    "In this project, we'll create a language detection model using a parameter-efficient model. This model will leverage the **DistillBert** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRa\n",
    "* Model: distilbert\n",
    "* Evaluation approach: Accuracy\n",
    "* Fine-tuning dataset: papluca/language-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5447a74",
   "metadata": {},
   "source": [
    "### Download Language Detection dataset\n",
    "We download the Language Detection dataset. We download 3 data splits: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the datasets and transformers packages\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"papluca/language-identification\")\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(5000))\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6101b5",
   "metadata": {},
   "source": [
    "Let's take a peak at the first 5 records of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b4a149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['el', 'de', 'bg', 'de', 'vi'],\n",
       " 'text': ['ŒöŒ±Œπ, œÜœÖœÉŒπŒ∫Œ¨, œÑŒ± œÉœÄŒøœÖŒ¥Œ±ŒØŒ± ŒºŒΩŒ∑ŒºŒµŒØŒ± œÑŒ∑œÇ Œ∫Œ±œÑŒ±ŒΩœåŒ∑œÉŒ∑œÇ œÑŒ∑œÇ ŒµŒªŒµœÖŒ∏ŒµœÅŒØŒ±œÇ Œ±œÄœå œÑŒøŒΩ Œ¥Œ≠Œ∫Œ±œÑŒø œåŒ≥Œ¥ŒøŒø Œ±ŒπœéŒΩŒ± ŒµŒØŒΩŒ±Œπ œÑŒø Œ£œçŒΩœÑŒ±Œ≥ŒºŒ± Œ∫Œ±Œπ Œø ŒùœåŒºŒøœÇ œÑœâŒΩ ŒîŒπŒ∫Œ±ŒπœâŒºŒ¨œÑœâŒΩ.',\n",
       "  'Wie schon jemand bem√§ngelte, hat die gelieferte Sitzauflage KEINE Ecken- und keine seitliche Randabdeckung, wie auf dem Bild. Auch ist die Sitzauflage entweder mit nur sehr wenig oder gar keiner Bambus-Holzkohle gef√ºllt, verglichen mit den Auflagen, die ich fr√ºher bei einem anderen Anbieter bestellt habe.',\n",
       "  '–µ –∫–∞—Ç–æ –∫–æ–ª–µ–∫—Ü–∏—è –æ—Ç –∫–∏–±—Ä–∏—Ç–∏',\n",
       "  'Sehr sch√∂ne Kugel. Am 10. Oktober gekauft und am 20. Oktober geliefert üò≥ Als nette Zugabe befand sich noch ein Deichmann Prospekt in meinem P√§ckchen üò≥ Primavera und Deichmann ? Muss ich das verstehen? F√ºr die Lieferzeit und den konventionellen Prospekt jeweils 1 1/2 Punktabz√ºge . Werde dort sicher nicht mehr bestellen',\n",
       "  'M·ªôt v√†i t·∫ßng ƒëang ch√°y c√≥ l·∫Ω n·∫±m ngo√†i kh·∫£ nƒÉng d·∫≠p t·∫Øt c·ªßa ƒë·ªôi c·ª©u h·ªèa m√† ch√∫ng ta c√≥ th·ªÉ x·ª≠ l√Ω.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds['test'][:5]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78e9bb",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "We import a tokenizer to tokenize the text data provided as input. We also take a sample record to verify that the tokenized data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([  101,  1181, 15290, 29744,  1184, 10260,  1196, 29746, 14150, 29745,\n",
      "        15290, 19865, 25529, 10260, 29745,  1197, 16856, 10325, 29746,  1188,\n",
      "        29436, 10325,  1194, 15290, 18947, 22919, 10260, 29741, 14150, 19865,\n",
      "         1010,  1193, 29746, 10325, 29747, 10260, 29750,  1195, 10260, 29740,\n",
      "        14150, 22919, 10260, 22919, 10260,  1188,  1197, 10260, 29745,  1010,\n",
      "         1192, 10260,  1186, 15290, 19865, 22919, 10260,  1182,  1189, 15290,\n",
      "        29436, 10325,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=False)\n",
    "\n",
    "# add pad_token to this tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "str_to_int={\"ar\": 0, \"bg\":1, \"de\":2, \"el\":3, \"en\":4, \"es\":5, \"fr\":6, \"hi\":7, \"it\":8, \"ja\":9, \"nl\":10, \\\n",
    "              \"pl\":11, \"pt\":12, \"ru\":13, \"sw\":14, \"th\":15, \"tr\":16, \"ur\":17, \"vi\":18, \"zh\":19}\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"Preprocess the dataset by returning tokenized examples.\"\"\"\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_batch[\"labels\"] = [str_to_int[label] for label in batch[\"labels\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "    tokenized_ds[split] = tokenized_ds[split].rename_column(\"labels\", \"label\")\n",
    "\n",
    "#converting dataset to Torch Tensor and define expected col labels. This is needed to train the Lora model.\n",
    "tokenized_ds['train'].set_format('torch', columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_ds['test'].set_format('torch', columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "# Show the first example of the tokenized training set\n",
    "print(tokenized_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01095c86",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "We import the transformer model. In our case, we decided to use the **DistillBert** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=20, # because we have 20 languages\n",
    "    id2label={0: \"ar\", 1: \"bg\", 2: \"de\", 3:\"el\", 4:\"en\", 5:\"es\", 6:\"fr\", 7:\"hi\", 8:\"it\", 9:\"ja\", 10:\"nl\", \\\n",
    "              11: \"pl\", 12:\"pt\", 13:\"ru\", 14:\"sw\", 15:\"th\", 16:\"tr\", 17:\"ur\", 18:\"vi\", 19:\"zh\"}, \n",
    "    label2id={\"ar\": 0, \"bg\":1, \"de\":2, \"el\":3, \"en\":4, \"es\":5, \"fr\":6, \"hi\":7, \"it\":8, \"ja\":9, \"nl\":10, \\\n",
    "              \"pl\":11, \"pt\":12, \"ru\":13, \"sw\":14, \"th\":15, \"tr\":16, \"ur\":17, \"vi\":18, \"zh\":19}\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e846c6",
   "metadata": {},
   "source": [
    "### Let's assess the pre-trained model using the accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0def71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate base model\n",
    "import numpy as np\n",
    "from transformers import Trainer, DataCollatorWithPadding, TrainingArguments\n",
    "\n",
    "#compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./regular\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# eval loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f44f3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.983672857284546,\n",
       " 'eval_accuracy': 0.094,\n",
       " 'eval_runtime': 55.9503,\n",
       " 'eval_samples_per_second': 89.365,\n",
       " 'eval_steps_per_second': 11.171}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf8307",
   "metadata": {},
   "source": [
    "We can see that the accuracy of the pre-trained model is pretty low (**9.4%**). Let's fine-tune the model using the LoRa approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43391058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,658,536 || all params: 69,984,552 || trainable%: 5.227633664069179\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_lin', 'k_lin','v_lin', 'lin1', 'lin2', 'classifier', 'pre-classifier'],\n",
    "    bias=\"lora_only\",\n",
    "    modules_to_save=[\"decode_head\"],\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3acb1e6",
   "metadata": {},
   "source": [
    "We can see that only **5.22%** of the parameters will be trained using the LoRa approach. This is much more efficient than re-training all the weights. Below we can see which parameters will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7edb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias torch.Size([3072])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.lora_B.default.weight torch.Size([3072, 32])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias torch.Size([768])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.lora_A.default.weight torch.Size([32, 3072])\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.lora_B.default.weight torch.Size([768, 32])\n",
      "base_model.model.pre_classifier.original_module.weight torch.Size([768, 768])\n",
      "base_model.model.pre_classifier.original_module.bias torch.Size([768])\n",
      "base_model.model.pre_classifier.modules_to_save.default.weight torch.Size([768, 768])\n",
      "base_model.model.pre_classifier.modules_to_save.default.bias torch.Size([768])\n",
      "base_model.model.classifier.original_module.weight torch.Size([20, 768])\n",
      "base_model.model.classifier.original_module.bias torch.Size([20])\n",
      "base_model.model.classifier.original_module.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.classifier.original_module.lora_B.default.weight torch.Size([20, 32])\n",
      "base_model.model.classifier.modules_to_save.default.weight torch.Size([20, 768])\n",
      "base_model.model.classifier.modules_to_save.default.bias torch.Size([20])\n",
      "base_model.model.classifier.modules_to_save.default.lora_A.default.weight torch.Size([32, 768])\n",
      "base_model.model.classifier.modules_to_save.default.lora_B.default.weight torch.Size([20, 32])\n"
     ]
    }
   ],
   "source": [
    "# confirm that only the LoRa paramaters are trainable\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 09:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.074963</td>\n",
       "      <td>0.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.081139</td>\n",
       "      <td>0.985600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.1348687864687643, metrics={'train_runtime': 573.0529, 'train_samples_per_second': 17.45, 'train_steps_per_second': 2.181, 'total_flos': 1414126275932160.0, 'train_loss': 0.1348687864687643, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the lora model\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/distillbert-languages-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,#4,\n",
    "    per_device_eval_batch_size=8,#2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"distillbert-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning.\n",
    "\n",
    "Let's load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distillbert-lora\", num_labels=20).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebfa78",
   "metadata": {},
   "source": [
    "Let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ebfc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08113910257816315,\n",
       " 'eval_accuracy': 0.9856,\n",
       " 'eval_runtime': 67.8712,\n",
       " 'eval_samples_per_second': 73.669,\n",
       " 'eval_steps_per_second': 9.209,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977aa189",
   "metadata": {},
   "source": [
    "We can see that the PEFT model is performing much better than the pre-trained model. As a reminder, it only had a **9.4%** accuracy while the PEFT model shows a **98.6%** accuracy. Arguably, we didn't actually train the pre-trained model, we only used it as is to detect the languages. However, by using the LoRa approach, we managed to train the model without having to update all the parameters. We only updated **5%** of those parameters. So, this is a major improvement and at low cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cfab0",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Let's apply the PEFT model on a couple of random examples:\n",
    "* \"It's a beautiful day in the park today.\" (English)\n",
    "* \"Je pense, donc je suis.\" (French)\n",
    "* \"Vamos a la playa!\" (Spanish)\n",
    "* \"Goede morgen, hoe gaat het?\" (Dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e127452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"'It's a beautiful day in the park today.\",\n",
    "               \"Je pense, donc je suis\",\n",
    "               \"Vamos a la playa!\",\n",
    "               \"Goede morgen, hoe gaat het?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en']\n",
      "['fr']\n",
      "['es']\n",
      "['nl']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=False)\n",
    "\n",
    "for test_string in test_strings:\n",
    "    input_ids = tokenizer(test_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "    predicted_label_classes = logits.argmax(-1)\n",
    "    predicted_labels = [model.config.id2label[predicted_label_classes.squeeze().tolist()]]\n",
    "    print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67953723",
   "metadata": {},
   "source": [
    "Success! The model predicted the correct language for each string. It confirms the high accuracy score achieved by the PEFT model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
